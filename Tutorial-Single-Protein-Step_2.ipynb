{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Network Analysis Tutorial\n",
    "\n",
    "# Single-Protein - Step 2\n",
    "\n",
    "In the second part of our generalized network analysis tutorial, the user is presented with another jupyter notebook where all the correlation maps calculated in the first part can be translated into molecular visualizations or plots. Here we provide an opportunity for users more comfortable with python programming to tailor the analysis to their specific scientific questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:09:43.829281Z",
     "start_time": "2020-06-04T23:09:34.192482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the python package\n",
    "import dynetan as dna\n",
    "\n",
    "from dynetan.toolkit import getNodeFromSel, getSelFromNode, getPath\n",
    "from dynetan.viz import viewPath, getCommunityColors\n",
    "from dynetan.viz import showCommunityByID, showCommunityByNodes\n",
    "from dynetan.viz import prepTclViz\n",
    "\n",
    "# Load auxiliary packages for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import networkx as nx\n",
    "import MDAnalysis as mda\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "\n",
    "# For visualization packages\n",
    "import nglview    as nv\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:09:45.368030Z",
     "start_time": "2020-06-04T23:09:45.364339Z"
    }
   },
   "outputs": [],
   "source": [
    "dnad = dna.datastorage.DNAdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:09:45.804654Z",
     "start_time": "2020-06-04T23:09:45.609293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Allows better plotting inside jupyter notebooks\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:09:46.385284Z",
     "start_time": "2020-06-04T23:09:46.083635Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:09:48.851893Z",
     "start_time": "2020-06-04T23:09:46.501424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure all the following packages are installed in your R distribution\n",
    "\n",
    "%R require(data.table)\n",
    "%R require(ggplot2)\n",
    "%R require(ggrepel)\n",
    "%R require(gdata)\n",
    "%R require(RColorBrewer)\n",
    "%R require(colorRamps)\n",
    "%R require(rPref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files and system definitions (same as in \"ProcessTrajectory\" notebook) \n",
    "\n",
    "Make sure that all the paths are the same as in the **Step 1** notebook. Also, you might want to have analysis resutls saved to a new location, to avoid mixing results of different systems. If you are copying this tutorial to adapt to your own system, be careful to select new paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:22.898264Z",
     "start_time": "2020-06-04T23:10:22.892936Z"
    }
   },
   "outputs": [],
   "source": [
    "dataDir = \"./TutorialResults/\"\n",
    "\n",
    "# Path where results will be written (you may want plots and data files in a new location)\n",
    "workDir = \"./TutorialResults/AnalysisResults\"\n",
    "\n",
    "fileNameRoot = \"dnaData_SinglePtn\"\n",
    "fullPathRoot = os.path.join(dataDir, fileNameRoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:23.725216Z",
     "start_time": "2020-06-04T23:10:23.717418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creates the workdirectory, and Plots directory.\n",
    "import os\n",
    "if not os.path.exists(workDir):\n",
    "    os.makedirs(workDir)\n",
    "    os.makedirs(os.path.join(workDir, \"Plots\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:26.333160Z",
     "start_time": "2020-06-04T23:10:25.940523Z"
    }
   },
   "outputs": [],
   "source": [
    "dnad.loadFromFile(fullPathRoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load reduced trajectory, in case this notebook did not load ALL trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:27.286250Z",
     "start_time": "2020-06-04T23:10:27.123096Z"
    }
   },
   "outputs": [],
   "source": [
    "dcdVizFile = fullPathRoot + \"_reducedTraj.dcd\"\n",
    "pdbVizFile = fullPathRoot + \"_reducedTraj.pdb\"\n",
    "\n",
    "workUviz = mda.Universe(pdbVizFile, dcdVizFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are loading the first trajectory of the notebook, we need to create the dnad.nodesAtmSel structure to make selections on the universe based on node indices. This is mostly used for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:29.005597Z",
     "start_time": "2020-06-04T23:10:29.001742Z"
    }
   },
   "outputs": [],
   "source": [
    "# We add this to the object for ease of access.\n",
    "dnad.nodesAtmSel = workUviz.atoms[ dnad.nodesIxArray ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:29.211185Z",
     "start_time": "2020-06-04T23:10:29.202288Z"
    }
   },
   "outputs": [],
   "source": [
    "# trgtNodes = getNodeFromSel(\"segid \" + ligandSegID, dnad.nodesAtmSel, dnad.atomToNode)\n",
    "# trgtNode = getNodeFromSel(\"segid \" + ligandSegID + \" and name P\", dnad.nodesAtmSel, dnad.atomToNode)\n",
    "# print(trgtNodes, trgtNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Visualize Paths and Communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells will:\n",
    "\n",
    "- Open an NGL View window displaying the molecular system. If the molecular system is not loaded check for possible errors in loading the python modules.\n",
    "- Show the optimal path between two nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:30.517687Z",
     "start_time": "2020-05-29T04:01:29.627802Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://proteinformatics.charite.de/ngl/doc/#User_manual/Usage/Selection_language\n",
    "\n",
    "w = nv.show_mdanalysis(workUviz.select_atoms(\"all\"))\n",
    "\n",
    "w._remote_call(\"setSize\", target=\"Widget\", args=[\"800px\", \"600px\"])\n",
    "w.parameters = dict(theme='light')\n",
    "\n",
    "w.clear_representations()\n",
    "w.add_cartoon(\"protein\")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:30.560498Z",
     "start_time": "2020-05-29T04:01:30.523680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select an arbitrary node in the enzyme\n",
    "enzNode1 = getNodeFromSel(\"segid ENZY and resname GLU and resid 115\", dnad.nodesAtmSel, dnad.atomToNode)\n",
    "enzNode2 = getNodeFromSel(\"segid ENZY and resid 125\", dnad.nodesAtmSel, dnad.atomToNode)\n",
    "\n",
    "# Get the optimal path connecting the two nondes.\n",
    "\n",
    "optpath = getPath( enzNode1[0], enzNode2[0], dnad.nodesAtmSel, dnad.preds)\n",
    "\n",
    "viewPath(w, optpath, dnad.distsAll, dnad.maxDirectDist, dnad.nodesAtmSel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells will:\n",
    "- Open an NGL View window displaying the system.\n",
    "- Show the contacts with highst betweeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:31.633150Z",
     "start_time": "2020-05-29T04:01:30.563342Z"
    }
   },
   "outputs": [],
   "source": [
    "# http://proteinformatics.charite.de/ngl/doc/#User_manual/Usage/Selection_language\n",
    "\n",
    "w = nv.show_mdanalysis(workUviz.select_atoms(\"all\"))\n",
    "\n",
    "w._remote_call(\"setSize\", target=\"Widget\", args=[\"800px\", \"600px\"])\n",
    "w.parameters = dict(theme='light')\n",
    "\n",
    "w.clear_representations()\n",
    "w.add_cartoon(\"protein\")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:31.754040Z",
     "start_time": "2020-05-29T04:01:31.636810Z"
    }
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Plot the 10 highest betweennesses (edges most used by optimal paths)\n",
    "####\n",
    "\n",
    "w.clear_representations()\n",
    "w.add_cartoon(\"protein\")\n",
    "\n",
    "window = 0\n",
    "for k,v in islice(dnad.btws[window].items(),10):\n",
    "    print(k,v, dnad.corrMatAll[0, k[0], k[1]])\n",
    "    viewPath(w, getPath(k[0], k[1], dnad.nodesAtmSel, dnad.preds), \n",
    "             dnad.distsAll, dnad.maxDirectDist, dnad.nodesAtmSel, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:31.759732Z",
     "start_time": "2020-05-29T04:01:31.756567Z"
    }
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Save the current visualization in a figue.\n",
    "####\n",
    "\n",
    "#w.download_image(filename='nglview_frame.png', factor=15, trim=True, antialias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:31.854567Z",
     "start_time": "2020-05-29T04:01:31.763392Z"
    }
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Close the current visualization.\n",
    "####\n",
    "\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Communitites \n",
    "We keep the communities that have more than 1% of nodes in all windows. Then we group communities across replicas by largest intersection. This is needed because we have no guarantee that the same community will be assigned the same ID in different windows of the same simulation.\n",
    "\n",
    "We fin ally rank the communities by modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:35.825349Z",
     "start_time": "2020-06-04T23:10:35.722709Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx.algorithms.community.quality as nxquality\n",
    "\n",
    "# Creates a list of windows and order them according to graph modularity.\n",
    "windModul = []\n",
    "for window in range(dnad.numWinds):\n",
    "    modul = nxquality.modularity(dnad.nxGraphs[window], \n",
    "                         [ set(nodesList) for nodesList in dnad.nodesComm[window][\"commNodes\"].values()])\n",
    "    windModul.append((window, modul))\n",
    "    \n",
    "windModul.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# Keep the window with the highest modularity as a reference for community matching\n",
    "refWindow = windModul[0][0]\n",
    "\n",
    "for wind, mod in windModul[:5]:\n",
    "    print( \"Window {} has modularity {:1.4f}.\".format(wind, mod) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:36.127259Z",
     "start_time": "2020-06-04T23:10:36.094895Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff = max(10, np.ceil(0.01*dnad.numNodes))\n",
    "\n",
    "def matchComm(mCommID, mWindow, refWindow, dnad, cutoff=1):\n",
    "    \"\"\"\n",
    "    Returns the community ID for the reference window that has the largest\n",
    "    intersection with the matching community at the matching window.\n",
    "    Communities at the reference window with less than *cutoff* percent of nodes\n",
    "    are ignored.\n",
    "    \"\"\"\n",
    "    \n",
    "    trgtComm = -1\n",
    "    intersectSize = 0\n",
    "    for commID in dnad.nodesComm[refWindow][\"commOrderSize\"]:\n",
    "        # Skip community if it has less than one percent of the nodes.\n",
    "        commSize = len(dnad.nodesComm[refWindow][\"commNodes\"][commID])\n",
    "        if commSize < cutoff:\n",
    "            continue\n",
    "        \n",
    "        tmpSize = len( set(dnad.nodesComm[refWindow][\"commNodes\"][commID]).intersection( \n",
    "            set(dnad.nodesComm[mWindow][\"commNodes\"][mCommID]) ) )\n",
    "        \n",
    "        # Selects the largets intersection\n",
    "        if intersectSize < tmpSize:\n",
    "            intersectSize = tmpSize\n",
    "            trgtComm = commID\n",
    "    \n",
    "    return trgtComm, intersectSize\n",
    "\n",
    "print(\"Using reference window {0} with highest modularity {1:<1.4}\".format(*windModul[0]))\n",
    "\n",
    "communities = defaultdict(list)\n",
    "for window in range(dnad.numWinds):\n",
    "    for commID in dnad.nodesComm[window][\"commOrderSize\"]:\n",
    "        \n",
    "        # Skip community if it has less than one percent of the nodes.\n",
    "        commSize = len(dnad.nodesComm[window][\"commNodes\"][commID])\n",
    "        if commSize < cutoff:\n",
    "            continue\n",
    "        \n",
    "        matchID, interSize = matchComm(commID, window, refWindow, dnad, cutoff)\n",
    "        \n",
    "        communities[matchID].append( (commID, interSize, window) )\n",
    "        \n",
    "communities = {key:val for (key,val) in communities.items() }\n",
    "communities.keys()\n",
    "\n",
    "# Creates a list of communities ID from the dictionary keys\n",
    "# Orders the keys according to mean intersection size over all windows.\n",
    "tmpList = []\n",
    "for key,val in communities.items():\n",
    "    tmpList.append((key, np.mean([pair[1] for pair in val]), len(val)))\n",
    "tmpList.sort(key=lambda x:x[1], reverse=True)\n",
    "tmpList\n",
    "\n",
    "# Creates a pandas data frame for plotting and analysis\n",
    "commList = []\n",
    "genCommID = 0\n",
    "for key in [x[0] for x in tmpList]:\n",
    "    val = communities[key]\n",
    "    for valList in val:\n",
    "        commList.append( [genCommID, *valList ] )\n",
    "    genCommID += 1\n",
    "\n",
    "commDF = pd.DataFrame(data=commList, columns=[\"genCommID\",\"commID\",\"interSize\",\"Window\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:36.816115Z",
     "start_time": "2020-06-04T23:10:36.794165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changes \"genCommID\" for communities that are matched to the same community in the reference window.\n",
    "c = commDF.groupby([\"genCommID\",\"Window\"]).cumcount()\n",
    "c *= 0.1\n",
    "commDF[ \"genCommID\" ] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:37.185313Z",
     "start_time": "2020-06-04T23:10:37.122868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a NumPy 2D array to organize data and transform it in a pandas DF.\n",
    "# Not pretty but its pynthon...\n",
    "nodeCommNP = np.empty([dnad.numNodes, dnad.numWinds])\n",
    "nodeCommNP.fill(-1)\n",
    "\n",
    "#Group by general community ID\n",
    "grpBy = commDF.groupby(\"genCommID\")\n",
    "for genCommID, group in grpBy:\n",
    "    for winIndx,commID in group[[\"Window\",\"commID\"]].values:\n",
    "        for node in range(dnad.numNodes):\n",
    "            if dnad.nxGraphs[winIndx].nodes[node][\"modularity\"] == commID:\n",
    "                nodeCommNP[node, winIndx] = genCommID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:37.328246Z",
     "start_time": "2020-06-04T23:10:37.314017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removes nodes that were not classified in a \"big-nough\" (bigger than 1%) cluster in *any* window.\n",
    "nodeCommDF = pd.DataFrame(data=nodeCommNP,columns=[\"Window\"+str(i) for i in range(dnad.numWinds)])\n",
    "nodeCommDF[\"Node\"] = [i for i in range(dnad.numNodes)]\n",
    "nodeCommDF = nodeCommDF[ nodeCommDF.min(1) >= 0]\n",
    "# So we don't get \"blank\"/empty areas in the plot\n",
    "nodeCommDF[\"NodePlot\"] = [i for i in range(len(np.unique(nodeCommDF[\"Node\"])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:38.201629Z",
     "start_time": "2020-06-04T23:10:38.170785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Melts for plotting.\n",
    "nodeCommDFmelt = nodeCommDF.melt(id_vars=[\"Node\",\"NodePlot\"], value_name=\"Cluster\", var_name=\"Window\")\n",
    "# Makes it easier to plot\n",
    "nodeCommDFmelt[\"Cluster\"] = nodeCommDFmelt[\"Cluster\"].astype('category')\n",
    "# Makes it easier to plot\n",
    "for i in range(dnad.numWinds):\n",
    "    nodeCommDFmelt.replace(\"Window\"+str(i),i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:39.988933Z",
     "start_time": "2020-06-04T23:10:39.535287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add readable info to nodes\n",
    "def getTagStr(i):\n",
    "    # Store atom names for residues with multiple nodes\n",
    "    if len(getNodeFromSel( getSelFromNode(i, dnad.nodesAtmSel), dnad.nodesAtmSel, dnad.atomToNode)) > 1:\n",
    "        atmStr = \":\" + dnad.nodesAtmSel.atoms[i].name\n",
    "    else:\n",
    "        atmStr = \"\"\n",
    "        \n",
    "    retStr = dnad.nodesAtmSel.atoms[i].resname.capitalize() + \\\n",
    "            \":\" + str(dnad.nodesAtmSel.atoms[i].resid) + \\\n",
    "            atmStr + \\\n",
    "            \"_\" + dnad.nodesAtmSel.atoms[i].segid\n",
    "            \n",
    "    return retStr\n",
    "\n",
    "nodeCommDFmelt['resid']     = np.vectorize(getTagStr)(nodeCommDFmelt[\"Node\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:40.109538Z",
     "start_time": "2020-06-04T23:10:40.058119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write data for Ploting (plots from ggplot in R are much better!)\n",
    "nodeCommDFmelt.to_csv(os.path.join(workDir, \"cluster.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare pandas data frame with community data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:10:42.968288Z",
     "start_time": "2020-06-04T23:10:42.962222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepares variable names for multi-system comparisons. In this tutorial, we only have one system.\n",
    "system1 = \"ENZY\"\n",
    "cDF = nodeCommDFmelt\n",
    "cDF[\"system\"] = system1\n",
    "refWindow1 = refWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:11:39.531285Z",
     "start_time": "2020-06-04T23:11:39.498328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loads VMD-compatible color scales to match community colors in R plots, NGLView, and VMD figures.\n",
    "comColorScale = getCommunityColors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:11:40.648977Z",
     "start_time": "2020-06-04T23:11:40.577426Z"
    }
   },
   "outputs": [],
   "source": [
    "%%R -i workDir,refWindow,comColorScale -o colorValues,clusterIDs -w 800 -h 450\n",
    "\n",
    "dataPath = file.path(workDir, \"cluster.csv\")\n",
    "\n",
    "dt <- fread(dataPath)\n",
    "clusterIDs = dt[, unique(Cluster)]\n",
    "\n",
    "colourCount = length(unique(dt$Cluster))\n",
    "\n",
    "# We only have 50 availabl colors\n",
    "colourCount <- min(colourCount,50)\n",
    "\n",
    "rgbCodes <- data.table(comColorScale)\n",
    "\n",
    "colorValues <- sapply(seq(colourCount), function(x) rgb(rgbCodes[x, .(R,G,B) ],  maxColorValue = 255) )\n",
    "\n",
    "setorder(dt, Cluster)                      \n",
    "colorValues = setNames(colorValues, dt[, unique(Cluster)])\n",
    "colorValues           \n",
    "\n",
    "print(paste(\"Creating palette for\",colourCount,\"clusters\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:11:43.424320Z",
     "start_time": "2020-06-04T23:11:43.417006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Builds a color dictionary from the cluster color scale built in R\n",
    "colorValDict = {}\n",
    "colorValDictRGB = {}\n",
    "for key,val in zip(clusterIDs, list(colorValues)):\n",
    "    colorValDict[key] = val\n",
    "\n",
    "for key,val in colorValDict.items():\n",
    "    colorValDictRGB[key] = tuple(int(val.lstrip('#')[i:i+2], 16) for i in (0, 2 ,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View community in the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire communities:\n",
    "\n",
    "Here, we show all nodes from a chosen community. The following jupyter notebook cell different options to choose which community is shown. The first will select a community ID and show all nodes. The second allows you to select a set of nodes and show them in the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:35.748681Z",
     "start_time": "2020-05-29T04:01:34.811521Z"
    }
   },
   "outputs": [],
   "source": [
    "w = nv.show_mdanalysis(workUviz.select_atoms(\"all\"))\n",
    "\n",
    "w._remote_call(\"setSize\", target=\"Widget\", args=[\"800px\", \"600px\"])\n",
    "w.parameters = dict(theme='light')\n",
    "\n",
    "opac = 1\n",
    "\n",
    "w.clear_representations()\n",
    "w.add_cartoon(\"protein\", color='lightgray', opacity=opac)\n",
    "\n",
    "shapeCounter = [0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:36.095702Z",
     "start_time": "2020-05-29T04:01:35.751685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dysplays all nodes of a cluster and colors them by cluster ID.\n",
    "\n",
    "clusterID = 0\n",
    "showCommunityByID(w, cDF, clusterID, system1, refWindow, \n",
    "                  shapeCounter, dnad.nodesAtmSel, colorValDictRGB, system1, refWindow1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:36.755540Z",
     "start_time": "2020-05-29T04:01:36.098710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dysplays a given list of nodes and colors them by cluster ID.\n",
    "\n",
    "clusterID = 0\n",
    "nodeList = cDF.loc[ (cDF.system == system1) & (cDF.Window == refWindow1) & (cDF.Cluster == clusterID) ].Node.values\n",
    "\n",
    "showCommunityByNodes(w, cDF, nodeList, system1, refWindow1, shapeCounter, dnad.nodesAtmSel, colorValDictRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:44.454763Z",
     "start_time": "2020-05-29T04:01:36.758085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Color ALL nodes by community\n",
    "\n",
    "nodeList = cDF.loc[ (cDF.system == system1) & (cDF.Window == refWindow1) ].Node.values\n",
    "showCommunityByNodes(w, cDF, nodeList, system1, refWindow1, shapeCounter, dnad.nodesAtmSel, colorValDictRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:01:44.462432Z",
     "start_time": "2020-05-29T04:01:44.457746Z"
    }
   },
   "outputs": [],
   "source": [
    "w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make more plots: Clustering of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:11:51.340335Z",
     "start_time": "2020-06-04T23:11:51.335796Z"
    }
   },
   "outputs": [],
   "source": [
    "plotFilePrefix = \"Enzy_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:11:53.451259Z",
     "start_time": "2020-06-04T23:11:52.080711Z"
    }
   },
   "outputs": [],
   "source": [
    "%%R -i workDir,plotFilePrefix -w 800 -h 450\n",
    "\n",
    "dataPath = file.path(workDir, \"cluster.csv\")\n",
    "plotPath = file.path(workDir, paste0(\"Plots/\",plotFilePrefix,\"Clusters_Node_vs_Window.png\"))\n",
    "\n",
    "dt <- fread(dataPath)\n",
    "dt <- dt[,.(NodePlot,Window,Cluster)]\n",
    "dt <- dt[, Cluster := as.factor(Cluster) ]\n",
    "\n",
    "p <- ggplot(dt) + \n",
    "    geom_raster(aes(x=NodePlot, y=Window, fill=Cluster)) + \n",
    "    scale_fill_manual(values = colorValues) +\n",
    "    labs(x=\"Node\", y=\"Window\") +\n",
    "    theme_bw(base_size=20)\n",
    "\n",
    "ggsave(plotPath, p, device=\"png\")\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty Figures - Preparing files for VMD\n",
    "\n",
    "The next few steps will define the files that will be created for loading into VMD, a popular molecular visualization software.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Make sure you have all the files created for VMD. For instance, if you are not interested in allosteric communications and suboptimal paths, just select two arbitrary nodes so that the path files are created correctly. Alternatively, you may want to adapt the tcl scripts provided at the end of the notebook to your particular case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:02:12.717797Z",
     "start_time": "2020-05-29T04:02:08.892077Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# This will be used to look for the maximum and  minimum betweenness value in the graph.\n",
    "# The maximum value will be used ot normalize all betweenness values for better vizualization.\n",
    "# The minimum value will be used in case a betweenness value could not be assigned for a given edge,\n",
    "#    also helping visualization.\n",
    "\n",
    "# Initialize variable with high value.\n",
    "minimumBetweeness = 100\n",
    "# Initialize variable with low value.\n",
    "maximumBetweeness = -1\n",
    "\n",
    "for pair,btw in dnad.btws[winIndx].items():\n",
    "    if btw < minimumBetweeness:\n",
    "            minimumBetweeness = btw\n",
    "    if btw > maximumBetweeness:\n",
    "            maximumBetweeness = btw\n",
    "\n",
    "# Normalize the value.\n",
    "minimumBetweeness /= maximumBetweeness\n",
    "            \n",
    "for winIndx in range(dnad.numWinds):\n",
    "\n",
    "    normCorMat = copy.deepcopy( dnad.corrMatAll[winIndx,:,:] )\n",
    "    normCorMat /= normCorMat.max()\n",
    "    \n",
    "    ##########################################################################################\n",
    "    ### Create PDB file with the system in the first step of each window, for VMD vizualization.\n",
    "    \n",
    "    pdbVizFile = os.path.join(workDir, \n",
    "                            \"networkData_Structure_window_{}.pdb\".format(winIndx))\n",
    "    \n",
    "    # Calculate number of frames per window.\n",
    "    winLen = int(np.floor(workUviz.trajectory.n_frames/dnad.numWinds))\n",
    "    \n",
    "    # Positions the trajectory at the middle of each window.\n",
    "    workUviz.trajectory[(winIndx+1)*round(winLen/2)]\n",
    "    \n",
    "    with mda.Writer(pdbVizFile, multiframe=False, bonds=\"conect\", n_atoms=workUviz.atoms.n_atoms) as PDB:\n",
    "        PDB.write(workUviz.atoms)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    ### Create network data file with ALL edges and their normalized weights.\n",
    "    \n",
    "    fileName = os.path.join(workDir, \n",
    "                            \"networkData_AllEdges_window_{}.dat\".format(winIndx))\n",
    "    with open(fileName, \"w\") as outfile:\n",
    "\n",
    "        for pair in np.asarray( np.where( np.triu(normCorMat[:,:]) ) ).T:\n",
    "\n",
    "            node1 = pair[0]\n",
    "            node2 = pair[1]\n",
    "\n",
    "            # Get VMD indices for the atoms\n",
    "            pdbIndx1 = dnad.nodesAtmSel.atoms[node1].id -1\n",
    "            pdbIndx2 = dnad.nodesAtmSel.atoms[node2].id -1\n",
    "\n",
    "            string = \"{} {} {}\".format(pdbIndx1, pdbIndx2, normCorMat[ node1, node2])\n",
    "            \n",
    "            outfile.write( string + \"\\n\" )\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    ### Create network data file with ALL NODES, the maximum normalized weight of edges it belongs to,\n",
    "    ### and the community it belongs to.\n",
    "    \n",
    "    fileName = os.path.join(workDir, \n",
    "                            \"networkData_AllNodes_window_{}.dat\".format(winIndx))\n",
    "    with open(fileName, \"w\") as outfile:\n",
    "        \n",
    "        for node1 in range(dnad.numNodes):\n",
    "            \n",
    "            # Get the VMD index for the atom\n",
    "            pdbIndx1 = dnad.nodesAtmSel.atoms[node1].id -1\n",
    "            \n",
    "            # Get the community the node belongs to\n",
    "            community1 = int(nodeCommNP[node1, winIndx])\n",
    "            \n",
    "            # Find the node to which \"node1\" is connected with highest correlation.\n",
    "            node2 = np.where( normCorMat[node1,:] == normCorMat[node1,:].max() )[0][0]\n",
    "            \n",
    "            # Skip nodes not assigned to any community\n",
    "            if community1 < 0:\n",
    "                continue\n",
    "            \n",
    "            string = \"{} {} {}\".format(pdbIndx1, normCorMat[ node1, node2], community1)\n",
    "            \n",
    "            outfile.write( string + \"\\n\" )\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    ### Create network data file with INTRA-COMMUNITY edges and their normalized weights.\n",
    "    \n",
    "    fileName = os.path.join(workDir, \n",
    "                            \"networkData_IntraCommunities_window_{}.dat\".format(winIndx))\n",
    "    with open(fileName, \"w\") as outfile:\n",
    "        \n",
    "        for pair in np.asarray( np.where( np.triu(normCorMat[:,:]) ) ).T:\n",
    "            \n",
    "            node1 = pair[0]\n",
    "            node2 = pair[1]\n",
    "\n",
    "            # Checks if both nodes belong to the same community. \n",
    "            # If they don't, skip this edge. We only write intra-community edges in this file!\n",
    "            if nodeCommNP[node1, winIndx] != nodeCommNP[node2, winIndx] :\n",
    "                continue\n",
    "            \n",
    "            # If both nodes do not belong to any community (assigned to community -1), also skip the edge.\n",
    "            if nodeCommNP[node1, winIndx] < 0:\n",
    "                continue\n",
    "            \n",
    "            community1 = int(nodeCommNP[node1, winIndx])\n",
    "            \n",
    "            # Get VMD indices for the atoms\n",
    "            pdbIndx1 = dnad.nodesAtmSel.atoms[node1].id -1\n",
    "            pdbIndx2 = dnad.nodesAtmSel.atoms[node2].id -1\n",
    "\n",
    "            string = \"{} {} {} {}\".format(pdbIndx1, pdbIndx2, normCorMat[ node1, node2], community1)\n",
    "            \n",
    "            outfile.write( string + \"\\n\" )\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    ### Create network data file with INTER-COMMUNITY edges and their normalized weights.\n",
    "    \n",
    "    fileName = os.path.join(workDir, \n",
    "                                \"networkData_InterCommunities_window_{}.dat\".format(winIndx))\n",
    "    with open(fileName, \"w\") as outfile:\n",
    "        \n",
    "        for pair in np.asarray( np.where( np.triu(normCorMat[:,:]) ) ).T:\n",
    "            \n",
    "            node1 = pair[0]\n",
    "            node2 = pair[1]\n",
    "\n",
    "            # Checks if both nodes belong to the same community. \n",
    "            # If they don't, skip this edge. We only write intra-community edges in this file!\n",
    "            if nodeCommNP[node1, winIndx] == nodeCommNP[node2, winIndx] :\n",
    "                continue\n",
    "            \n",
    "            # If either node does not belong to any community (assigned to community -1), also skip the edge.\n",
    "            if (nodeCommNP[node1, winIndx] < 0) or (nodeCommNP[node2, winIndx] < 0):\n",
    "                continue\n",
    "            \n",
    "            community1 = int(nodeCommNP[node1, winIndx])\n",
    "            community2 = int(nodeCommNP[node2, winIndx])\n",
    "            \n",
    "            # Get VMD indices for the atoms\n",
    "            # VMD uses a 0-based index, so we subtract 1 from the PDB index\n",
    "            pdbIndx1 = dnad.nodesAtmSel.atoms[node1].id -1\n",
    "            pdbIndx2 = dnad.nodesAtmSel.atoms[node2].id -1\n",
    "\n",
    "            string = \"{} {} {} {} {}\".format(pdbIndx1, pdbIndx2, \n",
    "                                             normCorMat[ node1, node2], community1, community2)\n",
    "            \n",
    "            outfile.write( string + \"\\n\" )\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    ### Create file with edges listed by betweeness value (highest to lowest).\n",
    "    \n",
    "    fileName = os.path.join(workDir, \n",
    "                            \"networkData_Betweenness_window_{}.dat\".format(winIndx))\n",
    "    with open(fileName, \"w\") as outfile:\n",
    "        \n",
    "        for pair,btw in dnad.btws[winIndx].items():\n",
    "            \n",
    "            node1 = pair[0]\n",
    "            node2 = pair[1]\n",
    "\n",
    "            # If either node does not belong to any community (assigned to community -1), also skip the edge.\n",
    "            if (nodeCommNP[node1, winIndx] < 0) or (nodeCommNP[node2, winIndx] < 0):\n",
    "                continue\n",
    "            \n",
    "            community1 = int(nodeCommNP[node1, winIndx])\n",
    "            community2 = int(nodeCommNP[node2, winIndx])\n",
    "            \n",
    "            # Get VMD indices for the atoms\n",
    "            # VMD uses a 0-based index, so we subtract 1 from the PDB index\n",
    "            pdbIndx1 = dnad.nodesAtmSel.atoms[node1].id -1\n",
    "            pdbIndx2 = dnad.nodesAtmSel.atoms[node2].id -1\n",
    "\n",
    "            string = \"{} {} {} {} {} {}\".format(pdbIndx1, pdbIndx2, \n",
    "                                                normCorMat[ node1, node2], btw/maximumBetweeness, \n",
    "                                                community1, community2)\n",
    "            \n",
    "            outfile.write( string + \"\\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Optimal and Sub-Optimal Paths\n",
    "\n",
    "Using the convenience functions \"getNodeFromSel\" and \"getSelFromNode\", one can easily probe the system and determine the relationship between node in the network graph and the atoms and residues they represent in the actual system.\n",
    "\n",
    "- getSelFromNode\n",
    "- getNodeFromSel\n",
    "\n",
    "See examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:02:12.730219Z",
     "start_time": "2020-05-29T04:02:12.720388Z"
    }
   },
   "outputs": [],
   "source": [
    "srcNode = getNodeFromSel(\"resname VAL and resid 11 and segid ENZY\",dnad.nodesAtmSel, dnad.atomToNode)[0]\n",
    "print(\"Source node:\", srcNode)\n",
    "\n",
    "trgNode = getNodeFromSel(\"resname ALA and resid 69 and segid ENZY\",dnad.nodesAtmSel, dnad.atomToNode)[0]\n",
    "print(\"Target node:\", trgNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:02:12.848687Z",
     "start_time": "2020-05-29T04:02:12.732788Z"
    }
   },
   "outputs": [],
   "source": [
    "getSelFromNode(58,dnad.nodesAtmSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a list of important paths:**\n",
    "\n",
    "Once you have chosen the nodes that define each path of interest, create a list in the cell below with the indices of the source and target nodes.\n",
    "\n",
    "To make sure that the VMD scripts will run without the need to be adapted, the user must select at least one pair of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:02:12.959829Z",
     "start_time": "2020-05-29T04:02:12.852749Z"
    }
   },
   "outputs": [],
   "source": [
    "# For example, to write the paths between node 0 (Valine 11) and nodes 58 and 60, the following\n",
    "#   list must be created:  \n",
    "# nodesForPaths = [ [0,58], [0,60] ]\n",
    "#\n",
    "\n",
    "nodesForPaths = [ [0,58], [0,60] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T04:02:13.365287Z",
     "start_time": "2020-05-29T04:02:12.962583Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Determine how many extra sub-optimal paths will be written.\n",
    "numSuboptimalPaths = 5\n",
    "\n",
    "pathListFile = open(os.path.join(workDir, \"paths.list\"), \"w\")\n",
    "\n",
    "for srcNode, trgNode in nodesForPaths:\n",
    "    \n",
    "    tmpList = getSelFromNode(srcNode,dnad.nodesAtmSel, atom=True).split()\n",
    "    srcNodeSel = \"\".join([tmpList[1],tmpList[4],tmpList[10]])\n",
    "\n",
    "    tmpList = getSelFromNode(trgNode,dnad.nodesAtmSel, atom=True).split()\n",
    "    trgNodeSel = \"\".join([tmpList[1],tmpList[4],tmpList[10]])\n",
    "    \n",
    "    # Adds the path suffix to the file\n",
    "    pathListFile.write(\"_{}_{}\\n\".format(srcNodeSel, trgNodeSel))\n",
    "    \n",
    "    for winIndx in range(dnad.numWinds):\n",
    "\n",
    "        normCorMat = copy.deepcopy( dnad.corrMatAll[winIndx,:,:] )\n",
    "        normCorMat /= normCorMat.max()\n",
    "\n",
    "        ##########################################################################################\n",
    "        ### Create file with edges listed by betweeness value (highest to lowest).\n",
    "        \n",
    "        # File name is created based on selections, not node index, for readability.\n",
    "        \n",
    "        \n",
    "        \n",
    "        fileName = os.path.join(workDir, \n",
    "                                    \"networkData_Paths_window_{}_{}_{}.dat\".format(winIndx, \n",
    "                                                                                srcNodeSel, trgNodeSel))\n",
    "        with open(fileName, \"w\") as outfile:\n",
    "\n",
    "            allPaths = []\n",
    "\n",
    "            # Reconstructs the optimal path from Floyd-Warshall algorithm\n",
    "            pathFW = nx.reconstruct_path(srcNode, trgNode, dnad.preds[winIndx])\n",
    "\n",
    "            allPaths.append(pathFW)\n",
    "\n",
    "            # Behind the scenes, use Dijkstra algorithm to find sub-optimal paths\n",
    "            for pathSO in islice(nx.shortest_simple_paths(dnad.nxGraphs[0], \n",
    "                                                srcNode, trgNode, weight=\"dist\"), 1, numSuboptimalPaths + 1):\n",
    "                allPaths.append(pathSO)\n",
    "\n",
    "            # Create a counter of number of paths that go though each edge, among all (sub-)optimal path(s).\n",
    "            pathCounter = defaultdict(int)\n",
    "            for pathIndx, pathIter in enumerate(allPaths):\n",
    "                # Iterate over edges in the path\n",
    "                for i in range(len(pathIter)-1):\n",
    "\n",
    "                    node1 = pathIter[i]\n",
    "                    node2 = pathIter[i+1]\n",
    "\n",
    "                    pathCounter[(node1, node2)] += 1\n",
    "\n",
    "            # Normalize the count\n",
    "            maxCount = np.max(list(pathCounter.values()))\n",
    "            for pair, count in pathCounter.items():\n",
    "                pathCounter[pair] = count/maxCount\n",
    "\n",
    "            for pathIndx, pathIter in enumerate(allPaths):\n",
    "                # Iterate over edges in the path\n",
    "                for i in range(len(pathIter)-1):\n",
    "\n",
    "                    node1 = pathIter[i]\n",
    "                    node2 = pathIter[i+1]\n",
    "\n",
    "                    # Get the community each node belongs to\n",
    "                    community1 = int(nodeCommNP[node1, winIndx])\n",
    "                    community2 = int(nodeCommNP[node2, winIndx])\n",
    "\n",
    "                    # If either node does not belong to any community (assigned to community -1), \n",
    "                    #     also skip the edge.\n",
    "                    if (community1 < 0) or (community2 < 0):\n",
    "                        continue\n",
    "\n",
    "                    # Get the betweeness value\n",
    "                    try:\n",
    "                        btw = dnad.btws[winIndx][( node1, node2)]\n",
    "                    except:\n",
    "                        # If one could not be calculated (very few paths going though this edge)\n",
    "                        # set an arbitrarily low value.\n",
    "                        btw = minimumBetweeness\n",
    "\n",
    "                    # Get VMD indices for the atoms\n",
    "                    # VMD uses a 0-based index, so we subtract 1 from the PDB index\n",
    "                    pdbIndx1 = dnad.nodesAtmSel.atoms[node1].id -1\n",
    "                    pdbIndx2 = dnad.nodesAtmSel.atoms[node2].id -1\n",
    "\n",
    "                    string = \"{} {} {} {} {} {}\".format(pdbIndx1, pdbIndx2, \n",
    "                                                     normCorMat[ node1, node2], \n",
    "                                                     btw/maximumBetweeness, pathCounter[(node1, node2)], \n",
    "                                                     pathIndx)\n",
    "\n",
    "                    outfile.write( string + \"\\n\" )\n",
    "\n",
    "pathListFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write TCL files to load all data we just wrote down and visualize in VMD.\n",
    "prepTclViz(\"networkData\", str(dnad.numWinds), \"NULL\", workDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering high-quality images with VMD - Step 3\n",
    "\n",
    "In the third step, the user can load files produced by the Jupyter notebooks into VMD. An easy to load script will handle all the work, creating a simple graphical  user  interface  (GUI)  that can be used to  easily  render publication-quality  images.   These  renderings  can  represent many aspects of the biomolecular system.\n",
    "\n",
    "There are *two options* to load the system into VMD and open the GUI:\n",
    "\n",
    "**1. From inside VMD.**\n",
    "\n",
    "    After opening VMD, in the VMD Main window, go to: Extensions > Tk Console. \n",
    "    \n",
    "    In the Tk Console, type the following command to navigate to the folder where the Analysis Results were saved (same as in the beginning of this tutorial Step 2): \n",
    "    \n",
    "`cd << path to folder >>`\n",
    "    \n",
    "    In the results folder, type the following command to load the results and the GUI: \n",
    "    \n",
    "`source network_view_2.tcl`\n",
    "\n",
    "**2. When loading VMD.**\n",
    "\n",
    "    If you choose to load VMD from a terminal (command line) window, navigate to the folder where the Analysis Results were saved (same as in the beginning of this tutorial Step 2): \n",
    "    \n",
    "`cd << path to folder >>`\n",
    "    \n",
    "    Load VMD with the following command: \n",
    "    \n",
    "`vmd -e network_view_2.tcl`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the Network View 2 GUI\n",
    "\n",
    "The Network View 2.0 GUI was created in a way that allows for easy interaction without deep background knowledge on VMD. If you are an expert VMD user, you can still change anything in the representation, but the GUI might erase your changes when loading some of the specialized features.\n",
    "\n",
    "The GUI allows user to visualize and render all the properties of the network presented in the Step 2. For instance:\n",
    "\n",
    "**1.** To visualize the communities just click on \"All Communities\". You can navigate diferent windows by clicking in the window \"step\" at the top-left corner. \n",
    "\n",
    "**2.** To load the betweenness just click on \"Betweenness\". You can, at the same time, visualize the communities of the protein nodes, by clicking on \"Show/Hide Colors\" in the \"Color Protein by Communities\" tab (only works for proteins).\n",
    "\n",
    "**3.** The \"Representations\" tab allows the user to show or hide parts of the structure.\n",
    "\n",
    "**4.** If you want to render the network in a higher or lower resolution, the \"Network Drawing Resultion\" tab provides options. Note that after selecting a new resolution, you must load the network representation again.\n",
    "\n",
    "**5.** Three options are available for quickly start rendering. The first two are GPU-only, and therefore depend on your computer having compatible GPU hardware. The third option uses CPU and should run in most computers. If you want to use a different rendering option, in the VMD Main window, go to: File > Render. Then, select the desired rendering option. **Attention** the quick menu for rendering will always save files with the same name. If you want to render multiple figures, and keep all of them, rename the just-rendered images to avoid overwriting the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:58:37.462224Z",
     "start_time": "2020-04-16T13:58:37.453579Z"
    }
   },
   "source": [
    "# ---- The End ----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
